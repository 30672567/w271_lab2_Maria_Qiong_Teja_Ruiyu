---
title: "Lab2_Co2_1997"
author: "Qiong Zhang"
date: "2024-03-10"
output: pdf_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE, echo=F}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(patchwork) 

library(lubridate)
library(datasets)

library(tsibble)
library(feasts)
library(forecast)

library(sandwich)
library(lmtest)
library(ggfortify)
library(nycflights13)
library(blsR)
library(ggplot2)

## Forecasting Models for Tidy Time Series
library(fable)
## To assemble multiple plots
library(gridExtra)
## for simulations 
library(simts)
## To use TeX() to write expression in the title of plots
library(latex2exp)
```

```{r set themes}
theme_set(theme_minimal())
```

# Report from the Point of View of 1997 

For the first part of this task, suspend reality for a short period of time and conduct your analysis from the point of view of a data scientist doing their work in the early months of 1998. Do this by using data that is included in _every_ R implementation, the `co2` dataset. This dataset is lazily loaded with every R instance, and is stored in an object called `co2`. 

## (3 points) Task 0a: Introduction 

Introduce the question to your audience. Suppose that they _could_ be interested in the question, but they don't have a deep background in the area. What is the question that you are addressing, why is it worth addressing, and what are you going to find at the completion of your analysis. Here are a few resource that you might use to start this motivation. 

- [Wikipedia](https://en.wikipedia.org/wiki/Keeling_Curve)
- [First Publication](./background/keeling_tellus_1960.pdf)
- [Autobiography of Keeling](./background/keeling_annual_review.pdf)

### The Keeling Curve
In the late 1950's, Charles Keeling initiated groundbreaking scientific endeavor, systematically measuring atmospheric carbon dioxide (CO2) levels in Mauna Loa, Hawaii (1). This work unveiled a striking pattern in the CO2 data, contradictory to the previous publications pointing to high variability (1)(2). Now known as the Keeling Curve, these findings have become an important reference in climate science.

### Analyzing Trends in CO2 data and Significance
Our analysis is centered on the critical question: *What trends can be identified in the CO2 data up to 1998, and what do they indicate about global environmental changes?* By examining the atmospheric concentrations of CO2 (parts per million (ppm)) collected at Mauna Loa, we aim to investigate the seasonal fluctuations in CO2 levels and the long term trend of increasing concentrations. Understanding this dynamic is important to discover insights into the natural cycles that regulate our planet's climate system and observe human impacts due to increase in fossil fuel and agriculture (2).  

### Aims and Implications of the Analysis
We aim to uncover the detailed patterns of CO2 variations and their correlation with both natural phenomena and anthropogenic factors. These findings will contribute a deeper understanding of CO2 impacts on Earth while potentially serving as a foundation for further studies and policy making. As we currently scrutinize the rising global temperatures and other various impacts of climate change, unveiling the story told by the Keeling Curve is not simply an observation of our past but a lens into our future.

Reference Numbers (To move to the end later): 
(1) Autobiography of Keeling
(2) First Publication

## (3 points) Task 1a: CO2 data
Conduct a comprehensive Exploratory Data Analysis on the `co2` series. This should include (without being limited to) a [description of how, where and why ](https://gml.noaa.gov/ccgg/about/co2_measurements.html) the data is generated, a thorough investigation of the trend, seasonal and irregular elements. Trends both in levels and growth rates should be discussed (consider expressing longer-run growth rates as annualized averages).

```{r}
# Load the CO2 dataset
data("co2")

# View the structure of the CO2 dataset
str(co2)

# Create a dataframe with co2 and date columns
co2_df <- data.frame(co2)
dates <- seq(as.Date("1959-01-01"), by = "month", length.out = length(co2))

# Format the dates to extract the month and year in "YYYY-MM" format
co2_df$monthyear <- format(dates, "%Y-%m")

# Convert to ts series
co2_df <- co2_df %>% 
  mutate(time_index = yearmonth(monthyear)) %>% # convert the date to year-month format
  as_tsibble(index = time_index) # create time series 

head(co2_df)
```
```{r, fig.width = 7, fig.height = 5, fig.align = 'center', warning = F}
# Time plot
timeplot_co2 <- co2_df %>%
  ggplot() +
  aes(x=time_index, y=co2) +
  geom_line() +
  labs(
    title = "Monthly Mean CO_2",
    x = "Month and Year",
    y = "CO_2 Parts per Million"
  )

# Histogram
hist_co2 <- co2_df %>%
  ggplot() +
  geom_histogram(aes(x = co2)) +
  labs(
    title = "Histogram of CO_2",
    x = "CO_2 Parts per Million"
  ) +
  theme(legend.position = c(.2, .8))

# ACF Plot
acf_co2 <- co2_df%>%
  ACF(y=co2) %>%
  autoplot()

# PACF Plot
pacf_co2 <- co2_df %>%
  ACF(y=co2, type = "partial") %>%
  autoplot() +
  labs(y = "PACF")

(timeplot_co2 + hist_co2) /
  (acf_co2 + pacf_co2)
```
**Interpretation**:\
We observed strong and persistent trend and seasonality in monthly mean CO_2. The ACF plots fails to converge to 0 and decay very slowly. The first lag of PACF is 1, suggesting that co2 has a unit root. The histogram is not strongly skewed and we can observe the linear trend in the time plot, so we think log transformation is not necessary in this case.

```{r kpss}
co2_df%>%
  mutate(diff_co2=difference(co2))%>%
  features(diff_co2, unitroot_kpss)
```
**Interpretation**:\
We used KPSS unit root test to find out the stationarity of difference of co2. The null hypothesis of KPSS is that the series is stationary. In the test, the first difference of co2 has p-value of 0.1, which is larger than 0.05, we fail to reject the null hypothesis of stationary and conclude that only one difference is required to make the co2 stationary.

```{r, fig.width = 7, fig.height = 5, fig.align = 'center', warning = F}
# First differencing of co2
timeplot_co2_1d <- co2_df %>%
  ggplot() +
  aes(x=time_index, y=difference(co2)) +
  geom_line() +
  labs(
    title = "1st Diff Monthly Mean CO_2",
    x = "Time Index",
    y = "1st Diff of Monthly Mean CO_2"
  )

# First differencing histogram
hist_co2_1d <- co2_df %>%
  ggplot() +
  geom_histogram(aes(x = difference(co2))) +
  labs(
    title = "1st Diff CO_2 Histogram",
    x = "1st Diff CO_2"
  ) +
  theme(legend.position = c(.2, .8))

# First differencing ACF
acf_co2_1d <- co2_df %>%
  ACF(y=difference(co2)) %>%
  autoplot()

# First differencing PACF
pacf_co2_1d <- co2_df %>%
  ACF(y=difference(co2), type = "partial") %>%
  autoplot()

(timeplot_co2_1d + hist_co2_1d) /
  (acf_co2_1d + pacf_co2_1d)
```
**Interpretation**:\
The time plot of the first differencing of monthly CO_2 indicates strong seasonality, which is further verified by the periodic oscillations in ACF plot. The ACF also suggests there are lag for non-seasonal MA since first lags are significant and lag of seasonal MA. The PACF suggests there are non-seasonal AR lags and maybe seasonal AR lags, but it is not clear.
  
```{r, fig.width = 7, fig.height = 5, fig.align = 'center', warning = F}

dcmp_co2 <- decompose(co2)
dcmp_co2_1d <- decompose(diff(co2))

plot(dcmp_co2)
plot(dcmp_co2_1d)
```
**Interpretation**:\
Since the magnitude of the seasonal fluctuations, or the variation around the trend-cycle, does not vary with the level of the time series, we applied the additive decomposition.\
Based on the decomposition plot, we found:\
1) The trend of monthly mean CO_2 is linear.\
2) The seasonal fluctuations of monthly mean CO_2 does not change with the level of the time series.\
3) The level of the first-differencing monthly mean CO_2 does not vary much over time.
```{r, fig.width = 7, fig.height = 5, fig.align = 'center', warning = F}
co2_df$trend <- dcmp_co2$trend
co2_df %>%
autoplot(co2, color = "grey") +
geom_line(data = co2_df, 
          aes(x = time_index, y = trend, color = "Trend"))+
  labs(title = expression(Trend~of~Monthly~Mean~CO[2]), 
       y = "CO2 Parts per Million",
       x = "Month and Year")
```





## (3 points) Task 2a: Linear time trend model

Fit a linear time trend model to the `co2` series, and examine the characteristics of the residuals. Compare this to a quadratic time trend model. Discuss whether a logarithmic transformation of the data would be appropriate. Fit a polynomial time trend model that incorporates seasonal dummy variables, and use this model to generate forecasts to the year 2020. 

We will first begin to decompose the data into a linear trend to see how well we are able to fit the data with just a linear time trend.
```{r}

mod.linear <- co2_df %>%
  model(trend_model = TSLM(co2 ~ trend()))

mod.quad <- co2_df %>%
  model(trend_model = TSLM(co2 ~ trend() + I(trend()^2)))

mod.linear %>% gg_tsresiduals() + labs(title="Figure 3 Linear Model Residuals")

```

```{r}
mod.quad %>% gg_tsresiduals() + labs(title="Figure 4 Quadratic Model Residuals")
```

After fitting the quadratic model, we can see that the residuals in Figure 4 compared to the residuals in Figure 3 are more stationary. We can also see in the that the histogram in Figure 4 looks more normal than Figure 3. The quadratic model seems to fit the data better than the linear model. However, there is still seasonality in the ACF that needs to be addressed. We will also take a look at additive and multiplicative decomposition to see if using the log of CO2 concentration will help in fitting the data.

```{r fig.width=10, fig.height=5, fig.align="center", warning=FALSE}

co2_df <- co2_df %>%
  mutate(log_co2 = log(co2))

dcmp_add <- co2_df %>%
  model(add = classical_decomposition(co2, type = "additive")) 

dcmp_multi <- co2_df %>%
  model(stl = STL(log_co2))

p33 <- components(dcmp_add) %>% autoplot() + labs("Figure 5 Classical Decomposition")

p34<- components(dcmp_add) %>%
  ACF(random) %>%
  autoplot() + labs(title="Residuals additive decomposition")

p35 <- components(dcmp_multi) %>% autoplot() + labs("Figure 6 STL Decomposition")

p36<- components(dcmp_multi)%>%
  ACF(remainder) %>%
  autoplot() + labs(title="Residuals of multiplicative decomposition")

grid.arrange(p33,p34,p35 ,p36, nrow = 2, ncol = 2)

```

When comparing at the residuals in Figure 5 (additive decomposition) and Figure 6 (multiplicative decomposition), we can looking at he ACFs that the residuals are less pronounced when we use multiplicative decomposition. Therefore by logging the values of CO2 we are able to better capture any non linear relationships that we have in our data. Next we will fit polynomial time trend model that incorporates seasonal dummy variables to capture the seasonality of the data. We will also be using the log of CO2 in our model.

```{r}
mod.quad_season <- co2_df %>%
  model(trend_model = TSLM(log_co2 ~ trend()+I(trend()^2)+ season())) 
mod.quad_season %>% gg_tsresiduals() + labs(title="Figure 7 Polynomial and Seasonal Dummy Variables Model Residuals")
```

Based on Figure 7, we can see that the seasonal dummy variables have done a much better job capturing the seasonality in our data than our previous models. The ACF shows no instance of seasonality. Based on the scale of the residuals, we can see that our data is better captured by the seasonal dummy variable, as the scales in the graph of the residuals is much smaller than the scale in Figures 3 and 4. Although we are capturing the data better, we do not have white noise as the correlations in the ACF as we increase the lag still seem to be significant. Next we will forecast using this model.

```{r}
mod.quad_season.predictions <- new_data(co2_df, n = 300)

mod.quad_season %>%
  forecast(new_data = mod.quad_season.predictions) %>%
  autoplot(co2_df) + labs(title = "Figure 8 Linear time trend model Forecast")
```

## (3 points) Task 3a: ARIMA times series model 

Following all appropriate steps, choose an ARIMA model to fit to the series. Discuss the characteristics of your model and how you selected between alternative ARIMA specifications. Use your model (or models) to generate forecasts to the year 2022. 

From above sections, we have proved that co2 has 1 unit root and lags in both non-seasonal and seasonal MA and AR models. So, our basic model will search MA(p:1-10) and AR(q: 1-10) with d=1. Also because the time plot of the first differencing of monthly CO_2 indicates strong seasonality, we will include intercept in the model. Then we will use ARIMA() to find out the exact number of lag by searching a set of different possible models, comparing AIC/BIC, and selecting the model with the lowest values.
```{r ARIMA}
co2_df %>%
  model(ARIMA(co2 ~ 1 + pdq(1:10,1:2,1:10) + PDQ(0:10,0:10,0:10), ic="bic", stepwise=F, greedy=F)) %>%report()
  
```
The optimal model based on pre-defined criteria is ARIMA(3,1,1)(0,0,2), which has non-seasonal lags for both MA(1) and AR(3) and seasonal lags for MA(1,2) model, which is close to what we guess and observed before. 

Then we use residual to check the model fitness. The Figure 9 shows that the histogram of residual close to normal distribution. However, the acf plot of residual shows significant lags and seasonality. The Ljung Box test also showed that the we can reject the null hypothesis and the data are not independently distributed and residual have some serial correlation over time and violate the assumptions for stationary time series. 

```{r original model residual}

co2_fit<-co2_df%>%model(arima_fit=ARIMA(co2~1+pdq(3,1,1)+PDQ(0,0,2), ic="bic", stepwise=F, greedy=F))

co2_fit%>% gg_tsresiduals()+ labs(title = "Figure 9 Original Model Residual")

co2_fit%>%resid()%>%
  as.ts()%>%
  Box.test(., lag=10, type="Ljung-Box")

```
Since we are looking at the month data and the linear model section already proved that the quadratic term is helpful to predict the trend, we will include higher order polynomial trend and move the intercept.
```{r revised model}
co2_df%>%model(arima_fit=ARIMA(co2~0+pdq(3,1,1)+PDQ(0,1,2), ic="bic", stepwise=F, greedy=F))%>%report()
```

The BIC of revised model is much lower than the original model. We will check the residual again for model fitness. The Figure 10 shows that the acf plot has only 1 significant lag and the histogram distribution close to normal. The Ljung Box test showed that we failed to reject the null hypothesis and the residual is independently and randomly distributed, indicating good fitness.

```{r}
co2_fit2<-co2_df%>%model(arima_fit=ARIMA(co2~0+pdq(3,1,1)+PDQ(0,1,2), ic="bic", stepwise=F, greedy=F))

co2_fit2%>% gg_tsresiduals()+ labs(title = "Figure 10 Revised Model Residual")

co2_fit2%>%resid()%>%
  as.ts()%>%
  Box.test(., lag=10, type="Ljung-Box")
```
In forecase, we used both original model and revised model to forecast. It looks like both model capture the general increasing trend of co2. But the revised model(Figure 12) also captured the fluctuation and seasonality within entire predicting period, while the original model(Figure 11) only capture the fluctuation in recent predicitng period.
```{r forecast}
co2_fit%>%
  forecast(h=300)%>%
  autoplot()+ labs(title = "Figure 11 Original Model Forecast")


co2_fit2%>%
  forecast(h=300)%>%
  autoplot()+ labs(title = "Figure 12 Revised Model Forecast")
```
## (3 points) Task 4a: Forecast atmospheric CO2 growth 

Generate predictions for when atmospheric CO2 is expected to be at [420 ppm](https://research.noaa.gov/article/ArtMID/587/ArticleID/2764/Coronavirus-response-barely-slows-rising-carbon-dioxide) and 500 ppm levels for the first and final times (consider prediction intervals as well as point estimates in your answer). Generate a prediction for atmospheric CO2 levels in the year 2100. How confident are you that these will be accurate predictions?

