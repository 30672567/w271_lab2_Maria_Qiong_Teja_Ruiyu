---
title: "combined"
output: html_document
---
---
title: "Lab2_Co2_1997"
author: "Qiong Zhang"
date: "2024-03-10"
output: pdf_document
header-includes:
   - \usepackage{ulem}
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE, echo=F}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(patchwork) 

library(lubridate)
library(datasets)

library(tsibble)
library(feasts)
library(forecast)

library(sandwich)
library(lmtest)
library(ggfortify)
library(nycflights13)
library(blsR)
library(ggplot2)

## Forecasting Models for Tidy Time Series
library(fable)
## To assemble multiple plots
library(gridExtra)
## for simulations 
## To use TeX() to write expression in the title of plots
library(latex2exp)
```

```{r set themes}
theme_set(theme_minimal())
```

# Report from the Point of View of 1997

For the first part of this task, suspend reality for a short period of time and conduct your analysis from the point of view of a data scientist doing their work in the early months of 1998. Do this by using data that is included in _every_ R implementation, the `co2` dataset. This dataset is lazily loaded with every R instance, and is stored in an object called `co2`. 

## (3 points) Task 0a: Introduction 

Introduce the question to your audience. Suppose that they _could_ be interested in the question, but they don't have a deep background in the area. What is the question that you are addressing, why is it worth addressing, and what are you going to find at the completion of your analysis. Here are a few resource that you might use to start this motivation. 

- [Wikipedia](https://en.wikipedia.org/wiki/Keeling_Curve)
- [First Publication](./background/keeling_tellus_1960.pdf)
- [Autobiography of Keeling](./background/keeling_annual_review.pdf)

\begin{center} \textbf{I. Background} \end{center}
\begin{center} \textit{A. The Keeling Curve} \end{center}
In the late 1950's, Charles Keeling initiated groundbreaking scientific endeavor, systematically measuring atmospheric carbon dioxide (CO2) levels in Mauna Loa, Hawaii (1). This work unveiled a striking pattern in the CO2 data, contradictory to the previous publications pointing to high variability (1)(2). Now known as the Keeling Curve, these findings have become an important reference in climate science.

\begin{center} \textit{B. Analyzing Trends in CO2 data and Significance} \end{center}
Our analysis is centered on the critical question: \textit{What trends can be identified in the CO2 data up to 1998, and what do they indicate about global environmental changes?} By examining the atmospheric concentrations of CO2 (parts per million (ppm)) collected at Mauna Loa, we aim to investigate the seasonal fluctuations in CO2 levels and the long term trend of increasing concentrations. Understanding this dynamic is important to discover insights into the natural cycles that regulate our planet's climate system and observe human impacts due to increase in fossil fuel and agriculture (2).  

\begin{center} \textit{C. Aims and Implications of the Analysis} \end{center}
We aim to uncover the detailed patterns of CO2 variations and their correlation with both natural phenomena and anthropogenic factors. These findings will contribute a deeper understanding of CO2 impacts on Earth while potentially serving as a foundation for further studies and policy making. As we currently scrutinize the rising global temperatures and other various impacts of climate change, unveiling the story told by the Keeling Curve is not simply an observation of our past but a lens into our future.

Reference Numbers (To move to the end later): 
(1) Autobiography of Keeling
(2) First Publication

## (3 points) Task 1a: CO2 data
Conduct a comprehensive Exploratory Data Analysis on the `co2` series. This should include (without being limited to) a [description of how, where and why ](https://gml.noaa.gov/ccgg/about/co2_measurements.html) the data is generated, a thorough investigation of the trend, seasonal and irregular elements. Trends both in levels and growth rates should be discussed (consider expressing longer-run growth rates as annualized averages).

```{r}
# Load the CO2 dataset
data("co2")

# View the structure of the CO2 dataset
str(co2)

# Create a dataframe with co2 and date columns
co2_df <- data.frame(co2)
dates <- seq(as.Date("1959-01-01"), by = "month", length.out = length(co2))

# Format the dates to extract the month and year in "YYYY-MM" format
co2_df$monthyear <- format(dates, "%Y-%m")

# Convert to ts series
co2_df <- co2_df %>% 
  mutate(time_index = yearmonth(monthyear)) %>% # convert the date to year-month format
  as_tsibble(index = time_index) # create time series 

head(co2_df)
frequency(co2_df)
frequency(co2_df$time_index)
```
```{r, fig.width = 7, fig.height = 5, fig.align = 'center', warning = F, include=F}
# Time plot
timeplot_co2 <- co2_df %>%
  ggplot() +
  aes(x=time_index, y=co2) +
  geom_line() +
  labs(
    title = "Monthly Mean CO_2",
    x = "Month and Year",
    y = "CO_2 Parts per Million"
  )

# Histogram
hist_co2 <- co2_df %>%
  ggplot() +
  geom_histogram(aes(x = co2)) +
  labs(
    title = "Histogram of CO_2",
    x = "CO_2 Parts per Million"
  ) +
  theme(legend.position = c(.2, .8))

# ACF Plot
acf_co2 <- co2_df%>%
  ACF(y=co2) %>%
  autoplot()

# PACF Plot
pacf_co2 <- co2_df %>%
  ACF(y=co2, type = "partial") %>%
  autoplot() +
  labs(y = "PACF")

(timeplot_co2 + hist_co2) /
  (acf_co2 + pacf_co2)
```
The charts above reflects a robust and consistent trend as well as seasonality within the monthly mean $CO_2$ data. Notably, the ACF plot demonstrated a lack of convergence to zero and exhibited slow decay, indicating pronounced autocorrelation persistence within the time series. This pattern suggests non-stationarity, wherein the mean, variance, and autocorrelation structure do not maintain constancy over time, implying that past values exert a lasting influence on future values. Furthermore, the first lag of the PACF yielded a value of 1, indicating the presence of a unit root within the $CO_2$ series. Additionally, the histogram is slightly skewed and a log transformation may be necessary
### interpret the result of KPSS test
```{r kpss, include=F, echo = FALSE}
co2_df%>%
  mutate(diff_co2=difference(co2))%>%
  features(diff_co2, unitroot_kpss)
```
We performed a KPSS unit root test to assess the stationarity of the first difference of $CO_2$. The null hypothesis of the KPSS test posits that the series is stationary. Our test results revealed a p-value of 0.1, greater than the conventional significance level of 0.05. Consequently, we fail to reject the null hypothesis of stationarity, leading the conclusion that the first differencing can achieve stationarity in the $CO_2$ series.

COMMENT: 1~2 to interpret the plots i.e. seasonality/ACF/PACF
```{r, fig.width = 7, fig.height = 5, fig.align = 'center', warning = F, include=F, echo = FALSE}
# First differencing of co2
timeplot_co2_1d <- co2_df %>%
  ggplot() +
  aes(x=time_index, y=difference(co2)) +
  geom_line() +
  labs(
    title = "1st Diff Monthly Mean CO_2",
    x = "Time Index",
    y = "1st Diff of Monthly Mean CO_2"
  )

# First differencing histogram
hist_co2_1d <- co2_df %>%
  ggplot() +
  geom_histogram(aes(x = difference(co2))) +
  labs(
    title = "1st Diff CO_2 Histogram",
    x = "1st Diff CO_2"
  ) +
  theme(legend.position = c(.2, .8))

# First differencing ACF
acf_co2_1d <- co2_df %>%
  ACF(y=difference(co2)) %>%
  autoplot()

# First differencing PACF
pacf_co2_1d <- co2_df %>%
  ACF(y=difference(co2), type = "partial") %>%
  autoplot()

(timeplot_co2_1d + hist_co2_1d) /
  (acf_co2_1d + pacf_co2_1d)
```
The time plot of the first difference of monthly $CO_2$ indicates strong seasonality, supported by periodic oscillations in the ACF plot. Significant lags for non-seasonal MA terms are observed, along with potential indications of seasonal MA terms. The PACF suggests the presence of non-seasonal and seasonal AR lags.
```{r, fig.width = 7, fig.height = 5, fig.align = 'center', warning = F, include=F, echo = FALSE}
dcmp_co2 <- decompose(co2)
dcmp_co2_1d <- decompose(diff(co2))

plot(dcmp_co2)
plot(dcmp_co2_1d)
```
Based on the decomposition plot, we observed a linear trend in the monthly mean CO₂, along with stable seasonal fluctuations that do not vary with the level of the time series. Additionally, the level of the first-differencing of monthly mean CO₂ demonstrates relatively consistent variation over time.
COMMENT:change the writting
```{r, fig.width = 7, fig.height = 5, fig.align = 'center', warning = F, include=F, echo = FALSE}
co2_df$trend <- dcmp_co2$trend
co2_df %>%
autoplot(co2, color = "grey") +
geom_line(data = co2_df, 
          aes(x = time_index, y = trend, color = "Trend"))+
  labs(title = expression(Trend~of~Monthly~Mean~CO[2]), 
       y = "CO2 Parts per Million",
       x = "Month and Year")
```
-----Comment: percentage & log growth rate are quite similar, only keep percentage growth rate plot
```{r, fig.width = 7, fig.height = 4, fig.align = 'center', warning = F}
co2_df_growth <- co2_df
co2_df_growth$year <- year(co2_df_growth$time_index)

co2_df_anuual_growth <- co2_df_growth %>%
  group_by(year) %>%
  slice(n())

co2_df_anuual_growth$lag_co2 <- lag(co2_df_anuual_growth$co2, 1)

co2_df_anuual_growth$growth_rate_pct <- (co2_df_anuual_growth$co2 / co2_df_anuual_growth$lag_co2 - 1) * 100
co2_df_anuual_growth$growth_rate_log <- log(co2_df_anuual_growth$co2 / co2_df_anuual_growth$lag_co2) * 100

co2_growth_rate_pct <- 
  ggplot(co2_df_anuual_growth, aes(x = time_index, y = growth_rate_pct)) +
  geom_line() +
  geom_smooth(method = "loess", se = TRUE, color = "blue", fill = "lightblue") +
  labs(title = "Percentage Growth Rate of CO2",
       x = "Date",
       y = "Growth Rate (%)")

co2_growth_rate_log <- 
  ggplot(co2_df_anuual_growth, aes(x = time_index, y = growth_rate_log)) +
  geom_line() +
  geom_smooth(method = "loess", se = TRUE, color = "blue", fill = "lightblue") +
  labs(title = "Log Growth Rate of CO2",
       x = "Date",
       y = "Growth Rate (%)")

co2_growth_rate_pct | co2_growth_rate_log
```
The percentage and logarithm annual growth rates of $CO_2$ levels are quite similar. We can observe that although the annual growth rate is below 0.8% which seems minimal, it does increase overtime with larger fluctuation.



## (3 points) Task 2a: Linear time trend model

Fit a linear time trend model to the `co2` series, and examine the characteristics of the residuals. Compare this to a quadratic time trend model. Discuss whether a logarithmic transformation of the data would be appropriate. Fit a polynomial time trend model that incorporates seasonal dummy variables, and use this model to generate forecasts to the year 2020. 

We will first begin to decompose the data into a linear trend to see how well we are able to fit the data with just a linear time trend.
```{r, fig.width = 7, fig.height = 4, fig.align = 'center', warning = F}

mod.linear <- co2_df %>%
  model(trend_model = TSLM(co2 ~ trend()))

mod.quad <- co2_df %>%
  model(trend_model = TSLM(co2 ~ trend() + I(trend()^2)))

mod.linear.res.plot <- mod.linear %>% gg_tsresiduals() + labs(title="Figure 3 Linear Model Residuals")

mod.quad.res.plot <- mod.quad %>% gg_tsresiduals() + labs(title="Figure 4 Quadratic Model Residuals")

mod.linear.res.plot 
mod.quad.res.plot
```
After fitting the quadratic model, we can see that the residuals in Figure 4 compared to the residuals in Figure 3 are more stationary. We can also see in the that the histogram in Figure 4 looks more normal than Figure 3. The quadratic model seems to fit the data better than the linear model. However, there is still seasonality in the ACF that needs to be addressed. We will also take a look at additive and multiplicative decomposition to see if using the log of CO2 concentration will help in fitting the data.
```{r fig.width=10, fig.height=5, fig.align="center", warning=FALSE}

co2_df <- co2_df %>%
  mutate(log_co2 = log(co2))

dcmp_add <- co2_df %>%
  model(add = classical_decomposition(co2, type = "additive")) 

dcmp_multi <- co2_df %>%
  model(stl = STL(log_co2))

p33 <- components(dcmp_add) %>% autoplot() + labs("Figure 5 Classical Decomposition")

p34<- components(dcmp_add) %>%
  ACF(random) %>%
  autoplot() + labs(title="Residuals additive decomposition")

p35 <- components(dcmp_multi) %>% autoplot() + labs("Figure 6 STL Decomposition")

p36<- components(dcmp_multi)%>%
  ACF(remainder) %>%
  autoplot() + labs(title="Residuals of multiplicative decomposition")

grid.arrange(p33,p34,p35 ,p36, nrow = 2, ncol = 2)

```

When comparing at the residuals in Figure 5 (additive decomposition) and Figure 6 (multiplicative decomposition), we can looking at he ACFs that the residuals are less pronounced when we use multiplicative decomposition. Therefore by logging the values of CO2 we are able to better capture any non linear relationships that we have in our data. Next we will fit polynomial time trend model that incorporates seasonal dummy variables to capture the seasonality of the data. We will also be using the log of CO2 in our model.

```{r}
mod.quad_season <- co2_df %>%
  model(trend_model = TSLM(log_co2 ~ trend()+I(trend()^2)+ season())) 
mod.quad_season %>% gg_tsresiduals() + labs(title="Figure 7 Polynomial and Seasonal Dummy Variables Model Residuals")
```
Based on Figure 7, we can see that the seasonal dummy variables have done a much better job capturing the seasonality in our data than our previous models. The ACF shows no instance of seasonality. Based on the scale of the residuals, we can see that our data is better captured by the seasonal dummy variable, as the scales in the graph of the residuals is much smaller than the scale in Figures 3 and 4. Although we are capturing the data better, we do not have white noise as the correlations in the ACF as we increase the lag still seem to be significant. Next we will forecast using this model.
```{r, fig.width = 7, fig.height = 4, fig.align = 'center', warning = F, include=F, echo = FALSE}
mod.linear_season <- co2_df %>%
  model(trend_model = TSLM(co2 ~ trend() + season())) 
mod.linear_season.aug <- mod.linear_season %>% augment()

mod.linear_season.aug.plot <- mod.linear_season.aug  %>%  
  autoplot(.fitted, color = "blue") +
  geom_line(data = co2_df, 
          aes(x = time_index, y = co2, color = "Actual Values"))+
  labs(title = expression(Fitted~Monthly~Mean~CO[2]~by~Linear~Model), 
       y = "CO2 Parts per Million",
       x = "Month and Year")

mod.quad_season.aug <- mod.quad_season %>% augment()
mod.quad_season.aug$rescale_fitted <- exp(mod.quad_season.aug$.fitted)

mod.quad_season.aug.plot <- mod.quad_season.aug %>%  
  autoplot(rescale_fitted, color = "blue") +
  geom_line(data = co2_df, 
          aes(x = time_index, y = co2, color = "Actual Values"))+
  labs(title = expression(Fitted~Monthly~Mean~CO[2]~by~Quadratic~Model~with~Log~Transformation), 
       y = "CO2 Parts per Million",
       x = "Month and Year")

mod.linear_season.aug.plot | mod.quad_season.aug.plot
```

```{r}
mod.quad_season.predictions <- new_data(co2_df, n = 300)

mod.quad_season %>%
  forecast(new_data = mod.quad_season.predictions) %>%
  autoplot(co2_df) + labs(title = "Figure 8 Linear time trend model Forecast")
```
## (3 points) Task 3a: ARIMA times series model 

Following all appropriate steps, choose an ARIMA model to fit to the series. Discuss the characteristics of your model and how you selected between alternative ARIMA specifications. Use your model (or models) to generate forecasts to the year 2022. 

From above sections, we have proved that co2 has 1 unit root and lags in both non-seasonal and seasonal MA and AR models. So, our basic model will search MA(p:0-3) and AR(q: 0-3) with d=1. The EDA of the first differencing of $CO_2$ indicates that the first differenced $CO_2$ data has strong seasonality while doesn't have persistent and obvious trend. The ACF further verified the yearly seasonality as the autocorrelation peaks at lag 12 and 24. Thus, we tested different ARIMA models below. Then we will use ARIMA() to find out the exact number of lag by searching a set of different possible models, comparing AIC/BIC, and selecting the model with the lowest values. We noticed that the model without intercept has much lower BIC than the model with intercept.
```{r, include=F, echo = FALSE}
# no intercept
co2_df %>%
  model(ARIMA(co2 ~ 0 + pdq(0:3,1:2,0:3) + PDQ(0:3,0:1,0:3, period=12), ic="bic", stepwise=F, greedy=F)) %>%report()
# with intercept
co2_df %>%
  model(ARIMA(co2 ~ 1 + pdq(0:3,1:2,0:3) + PDQ(0:3,0:1,0:3, period=12), ic="bic", stepwise=F, greedy=F)) %>%report()
```
The optimal model based on pre-defined criteria is ARIMA(0,1,1)(1,1,2), which has non-seasonal and seasonal difference 1,seasonal lag for AR(1), non-seasonal and seasonal lag for  MA(1,2) model, which is close to what we guess and observed before. 

Then we use residual to check the model fitness. The Figure 9 shows that the histogram of residual close to normal distribution. The acf plot of residual shows most lags within the limit with only 2 significant lags. The Ljung Box test also showed that the we cannot reject the null hypothesis (p=0.1441) and the data are independently distributed and residual does not have serial correlation over time and stationary. 

```{r original model residual, include=FALSE}

co2_fit<-co2_df%>%model(arima_fit=ARIMA(co2~pdq(0,1,1)+PDQ(1,1,2, period=12), ic="bic", stepwise=F, greedy=F))

co2_fit%>% gg_tsresiduals()+ labs(title = "Figure 9 Model Residual")

```
```{r, include=FALSE, echo=FALSE}
co2_fit%>%resid()%>%
  as.ts()%>%
  Box.test(., lag=10, type="Ljung-Box")
```

In forecast, it looks like our model capture the general increasing trend of co2 as well as the seasonality within entire predicting period (Figure 11).
```{r forecast, fig.width = 7, fig.height = 4, fig.align = 'center', warning = F}
co2_fit%>%
  forecast(h=300)%>%
  autoplot()+ labs(title = "Figure 11 ARIMA Model Forecast")
```


## (3 points) Task 4a: Forecast atmospheric CO2 growth 

COMMENT: add forecasting results for linear model, quadratic, arima in a single plot in different colors

Generate predictions for when atmospheric CO2 is expected to be at [420 ppm](https://research.noaa.gov/article/ArtMID/587/ArticleID/2764/Coronavirus-response-barely-slows-rising-carbon-dioxide) and 500 ppm levels for the first and final times (consider prediction intervals as well as point estimates in your answer). Generate a prediction for atmospheric CO2 levels in the year 2100. How confident are you that these will be accurate predictions?

```{r}
# Generate forecast till end of 2100
co2_fit.forecast <- co2_fit %>% forecast(h=1236)

# Plot the forecasting results
co2_fit.forecast %>%
  autoplot()+ labs(title = "Figure 10 Final Model Forecast")
```
```{r}
# Calculate the difference between 420
co2_fit.forecast$co2_420 <- co2_fit.forecast$.mean - 420

# Find the month year with minimum difference to 420
df_420 <- co2_fit.forecast %>% slice_min(abs(co2_420))
df_420
df_420$co2

# Extract the mean and standard deviation from the distribution
mean <- df_420$.mean
sd <- sqrt(242)

# Calculate the 95% confidence interval
lower_bound <- qnorm(0.025, mean, sd)  # 0.025 corresponds to the lower tail of the distribution
upper_bound <- qnorm(0.975, mean, sd)  # 0.975 corresponds to the upper tail of the distribution

# Print the confidence interval
print(c(lower_bound, upper_bound))
```
We calculate the difference between the point prediction (`.mean`) and 420, and extrac the row with lowest absolute difference. In this way, we can obtain the month and year of which $CO_2$ is closest to 420. Based on this approach, we found that in December 2033, the point prediction of $CO_2$ level is closest to 420, with variance equal to 242. We also calculate the 95% confidence interval, and we are 95% confident that the true $CO_2$ value is between 389.5 to 450.5.

```{r}
# Calculate the difference between 500
co2_fit.forecast$co2_500 <- co2_fit.forecast$.mean - 500

# Find the month year with minimum difference to 500
df_500 <- co2_fit.forecast %>% slice_min(abs(co2_500))

df_500
df_500$co2

# Extract the mean and standard deviation from the distribution
mean <- df_500$.mean
sd <- sqrt(2462)

# Calculate the 95% confidence interval
lower_bound_500 <- qnorm(0.025, mean, sd)  # 0.025 corresponds to the lower tail of the distribution
upper_bound_500 <- qnorm(0.975, mean, sd)  # 0.975 corresponds to the upper tail of the distribution

# Print the confidence interval
print(c(lower_bound_500, upper_bound_500))
```
We calculate the difference between the point prediction (`.mean`) and 500, and extrac the row with lowest absolute difference. In this way, we can obtain the month and year of which $CO_2$ is closest to 500. Based on this approach, we found that in June 2083, the point prediction of $CO_2$ level is closest to 420, with variance equal to 242. We also calculate the 95% confidence interval, and we are 95% confident that the true $CO_2$ value is between 402.8 to 597.3.
```{r}
co2_fit.forecast_2100 <- co2_fit.forecast[co2_fit.forecast$time_index >= as.Date("2100-01-01"), ]

# Plot the forecasting results
co2_fit.forecast_2100 %>%
  autoplot()+ labs(title = "Figure 14 Final Model Forecast for 2100")

# Range of point prediction
range(co2_fit.forecast_2100$.mean)
```
The point prediction of $CO_2$ levels in year 2100 ranges from 521.1 to 517.2. Throughout the entire year of 2100, we are 95% confident that the $CO_2$ levels fall within then  range of 400 to 600.

# Report from the Point of View of the Present 

One of the very interesting features of Keeling and colleagues' research is that they were able to evaluate, and re-evaluate the data as new series of measurements were released. This permitted the evaluation of previous models' performance and a much more difficult question: If their models' predictions were "off" was this the result of a failure of the model, or a change in the system? 

## (1 point) Task 0b: Introduction 

In this introduction, you can assume that your reader will have **just** read your 1997 report. In this introduction, **very** briefly pose the question that you are evaluating, and describe what (if anything) has changed in the data generating process between 1997 and the present.

\begin{center} \textbf{I. Background} \end{center}
\begin{center} \textit{A. Question at Hand} \end{center}
Following our 1997 report, we face a critical question: \textit{How accurately have past models predicted the CO2 levels measured since then?} This probes not just the precision of our forecast but also examining whether discrepancies signify model limitations or reflect shifts in the climate system.
\begin{center} \textit{B. Data Generation Process since 1997} \end{center}
Since 1997, significant advancements occurred in the data generating process for measuring CO2 levels at the Mauna Loa Observatory. There was an adoption of a new CO2 analyzer in April 2019, employing Cavity Ring-Down Spectroscopy (CRDS) technology, replacing the previous infrared absorption-based analyzer. Calibration methods also evolved, with meticulous control of temperature, pressure, and flow rate, along with frequent calibrations using reference gas mixtures. Furthermore, detailed data selection criteria have been implemented to identify background air, which aimed to eliminate local influences on CO2 measurements (3). In addition to the advancements, there was a disruption in measurements from November 2022 to July 2023 due to the eruption of the Mauna Loa Volcano, during which observations were conducted from the Maunakea Observatories approximately 21 miles north of the Mauna Loa Observatory. However, observations at Mauna Loa resumed in July 2023, ensuring continuity in the long-term CO2 monitoring efforts (4).

Reference, to move later
(3) https://gml.noaa.gov/ccgg/about/co2_measurements.html
(4) https://gml.noaa.gov/ccgg/trends/data.html

## (3 points) Task 1b: Create a modern data pipeline for Mona Loa CO2 data.

The most current data is provided by the United States' National Oceanic and Atmospheric Administration, on a data page [[here](https://gml.noaa.gov/ccgg/trends/data.html)]. Gather the most recent weekly data from this page. (A group that is interested in even more data management might choose to work with the [hourly data](https://gml.noaa.gov/aftp/data/trace_gases/co2/in-situ/surface/mlo/co2_mlo_surface-insitu_1_ccgg_HourlyData.txt).) 


Create a data pipeline that starts by reading from the appropriate URL, and ends by saving an object called `co2_present` that is a suitable time series object. 
```{r Data pipeline and reading data}
co2_present<-read.csv("https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.csv", header=T, sep=",", quote = "\"", dec = ".", fill = TRUE, comment.char = "#")
co2_present[1:4, ]
class(co2_present)
```


Conduct the same EDA on this data. Describe how the Keeling Curve evolved from 1997 to the present, noting where the series seems to be following similar trends to the series that you "evaluated in 1997" and where the series seems to be following different trends. This EDA can use the same, or very similar tools and views as you provided in your 1997 report. 

```{r EDA - dataset manipulation}
# Create a Date column from year, month, day - duplicates when only monthyear since its weekly data
co2_present$date <- as.Date(with(co2_present, paste(year, month, day, sep="-")), "%Y-%m-%d")
co2_present$time_index <- co2_present$date

# Convert to Time Series
co2_present <- co2_present %>%
  as_tsibble(index = time_index)
co2_present[1:5, ]

# Unknown values are indicated as -999.99.
# Counting number of -999.99 in each column
counts_NA <- list()
for (col_name in names(co2_present)) {
  counts_NA[[col_name]] <- sum(co2_present[[col_name]] == -999.99, na.rm = TRUE)
}
df_counts_NA <- as.data.frame(counts_NA, row.names = "Count_of_Neg999.99")
print(df_counts_NA)

# Mutating -999.99 in all columns to NA 
co2_present <- co2_present %>%
  mutate(across(c(average, X1.year.ago, X10.years.ago, increase.since.1800), ~na_if(.x, -999.99)))
```

```{r EDA - plots, warning = F}
# Time plot
timeplot_co2.p <- co2_present %>%
  ggplot() +
  aes(x=time_index, y=average) +
  geom_line() +
  labs(
    title = "Weekly Mean CO_2",
    x = "Time",
    y = "CO_2 Parts per Million"
  )

# Histogram
hist_co2.p <- co2_present %>%
  ggplot() +
  geom_histogram(aes(x = average)) +
  labs(
    title = "Histogram of CO_2",
    x = "CO_2 Parts per Million"
  ) +
  theme(legend.position = c(.2, .8))

# ACF Plot
acf_co2.p <- co2_present%>%
  ACF(y=average) %>%
  autoplot()

# PACF Plot
pacf_co2.p <- co2_present %>%
  ACF(y=average, type = "partial") %>%
  autoplot() +
  labs(y = "PACF")

(timeplot_co2.p + hist_co2.p) /
  (acf_co2.p + pacf_co2.p)
```
```{r EDA - plots, warning = F}
# Time plot
timeplot_co2_1d.p <- co2_present %>%
  ggplot() +
  aes(x=time_index, y=difference(average)) +
  geom_line() +
  labs(
    title = "Weekly Mean First Differenced CO_2",
    x = "Time",
    y = "CO_2 Parts per Million"
  )

# Histogram
hist_co2_1d.p <- co2_present %>%
  ggplot() +
  geom_histogram(aes(x = difference(average))) +
  labs(
    title = "Histogram of First Differenced CO_2",
    x = "CO_2 Parts per Million"
  ) +
  theme(legend.position = c(.2, .8))

# ACF Plot
acf_co2_1d.p <- co2_present%>%
  ACF(y=difference(average)) %>%
  autoplot()

# PACF Plot
pacf_co2_1d.p <- co2_present %>%
  ACF(y=difference(average), type = "partial") %>%
  autoplot() +
  labs(y = "PACF")

(timeplot_co2_1d.p + hist_co2_1d.p) /
  (acf_co2_1d.p + pacf_co2_1d.p)
```

The plots reveals a persistent upward trend in CO2 levels over time, mirroring findings from the 1997 report and indicating ongoing environmental concerns. Notably, the slow decay in the autocorrelation function suggests a sustained trend effect, while the first lag in the partial autocorrelation function indicates a unit root, highlighting a lack of stationarity in the data. These results are further supported by the KPSS test, which returns a p-value of 0.01, below the 5% threshold, leading to the rejection of the null hypothesis of stationarity. To address this issue, differencing the data was done, resulting in a subsequent KPSS test yielding a p-value of 0.1, indicating the necessity of differencing to achieve stationarity.

```{r EDA - KPSS original data}
co2_kpss<- co2_present %>%
  # Have to filter out any NAs before performing the KPSS test
  filter(!is.na(average)) %>%
  features(average, unitroot_kpss)
print(co2_kpss)
```
```{r EDA - KPSS differencing data}
# Differencing the data
co2_differenced <- co2_present %>%
  mutate(diff_average = difference(average))

co2_kpss_diff_results <- co2_differenced %>%
  features(diff_average, unitroot_kpss)
print(co2_kpss_diff_results)
```
```{r EDA - classical decomposition: original and differenced data}
# Omitting NAs for decomposition
co2_mod <- co2_present %>%
  mutate(average = na.approx(average, na.rm = FALSE))
class(co2_mod)
print(co2_mod)

# Classical Decomposition
co2_mod.ts <- ts(co2_mod$average, frequency = 52, start = c(1974, (as.numeric(format(min(co2_mod$date), "%j"))-1)/7 + 1))
class(co2_mod.ts)
co2_decomp <- decompose(co2_mod.ts, type = "additive")
plot(co2_decomp)

# Differenced data
diff_average_vector <- na.omit(co2_differenced$diff_average)
diff_average_ts <- ts(diff_average_vector, frequency = 52, start = c(1974, 2))
# Classical Decomposition
diff_average_decomp <- decompose(diff_average_ts, type = "additive")
plot(diff_average_decomp)
```
The time series decomposition graphs illustrate the transformation from non-stationary to stationary data. The original dataset depicts an upward trend, signifying non-stationarity, with clear seasonality and considerable random fluctuations. Post differencing, the trend component is neutralized, evidencing stationarity with a consistent mean. The seasonal patterns remain unchanged, indicating their persistence regardless of stationarity. The random component, though still volatile, is now centered around zero without a discernible trend, characterizing the achieved stationarity. 

```{r EDA - STL decomposition}
# Log transformation to average for STL
co2_mod <- co2_mod %>%
  as_tsibble(index = time_index)
co2_log_transformed <- co2_mod %>%
  mutate(log_average = log(average))

co2_log_transformed <- co2_log_transformed %>%
  mutate(time_index = as.Date(time_index)) %>%
  as_tsibble(index = time_index)

# STL decomposition
co2_stl <- co2_log_transformed %>%
  model(STL(log_average ~ season(window = "periodic")))
co2_components <- components(co2_stl)
autoplot(co2_components)
```


```{r EDA - 5 year moving average growth rate, warning = F}
# Calculating the average of the averages by each year
co2_annual_averages <- co2_present %>%
  index_by(year = year(time_index)) %>%
  summarise(annual_average = mean(average, na.rm = TRUE))

# Calculate the percentage growth rate based on the annual averages
co2_annual_averages <- co2_annual_averages %>%
  mutate(
    lag_annual_average = lag(annual_average),  # Lag
    Percentage_Growth_Rate = (annual_average - lag_annual_average) / lag_annual_average * 100
  ) %>%
  filter(!is.na(Percentage_Growth_Rate))

# Moving Average - 5 years
co2_annual_averages <- co2_annual_averages %>%
  mutate(
    Moving_Average_Growth_Rate = rollapply(Percentage_Growth_Rate, width = 5, FUN = mean, fill = NA, align = 'center', na.rm = TRUE)
  )

co2_growth_plot <- ggplot(co2_annual_averages, aes(x = year, y = Moving_Average_Growth_Rate)) +
  geom_line(color = "blue", na.rm = TRUE, show.legend = TRUE) +  
  geom_smooth(method = "loess", se = TRUE, color = "red", fill = 'lightblue', show.legend = TRUE) +  
  labs(title = "5-Year Moving Average Growth Rate of CO2 Levels",
       x = "Year",
       y = "5-Year Moving Avg Growth Rate (%)") +
  theme_minimal()

print(co2_growth_plot)

```
The 5-year moving average growth rate of CO2 levels smooths out the yearly fluctuations as shown by the blue line and displays a more consistent and interpretable trend with the red line. It can be obsered that from around 1980 until the early 2000s,the growth rate of CO2 levels increased moderately. However, after the early 2000s, there is a more pronounced upward trend, indicating that the rate at which CO2 is accumulating in the atmosphere has been accelerating.

```{r, create monthly data}
# Create a dataframe showing the monthly mean co2
monthly_mean_co2 <- co2_present[(!is.na(co2_present$average))
                                & (co2_present$year>1997), ] %>% 
  group_by(year, month) %>%
  index_by(monthyear = yearmonth(time_index)) %>%
  summarize(co2 = mean(average))

# Check the count
monthly_mean_co2[, c("year", "month")] %>% 
  group_by(year) %>%
  summarise(count = n())
colSums(is.na(monthly_mean_co2))

# Create a tsibble dataframe
monthly_mean_co2_df <- data.frame(monthyear  = monthly_mean_co2$monthyear, 
                                  year = monthly_mean_co2$year, 
                                  month = monthly_mean_co2$month, 
                                  co2 = monthly_mean_co2$co2)

# Convert it time series
monthly_mean_co2_df <- monthly_mean_co2_df %>%
  mutate(time_index = monthyear) %>%
  as_tsibble(index = time_index) 

class(monthly_mean_co2_df)

# Spot check
head(monthly_mean_co2_df)
frequency(monthly_mean_co2_df$time_index)
frequency(monthly_mean_co2_df)

# Time plot
monthly_mean_co2_df %>%
  ggplot() +
  aes(x=time_index, y=co2) +
  geom_line() +
  labs(
    title = "Actual Monthly Mean CO_2 after 1997",
    x = "Month and Year",
    y = "CO_2 Parts per Million"
  )

```

```{r, decomposition}
# Decomposition
co2_mod_monthly.ts <- ts(monthly_mean_co2_df$co2, frequency = 12)
class(co2_mod_monthly.ts)
co2_monthly_decomp <- decompose(co2_mod_monthly.ts, type = "additive")
plot(co2_monthly_decomp)
```

```{r, seasonal adjustment}
monthly_mean_co2_df$sa_co2 <- monthly_mean_co2_df$co2 - co2_monthly_decomp$seasonal

# plot nsa vs. sa
monthly_mean_co2_df %>%
  pivot_longer(cols = c(co2, sa_co2), 
               names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = time_index, y = value, color = variable)) +
  geom_line() +
  labs(title = "Plot of Seasonally Adjusted CO2 and Non-seasonally Adjusted CO2",
       x = "Date",
       y = "CO2 part per million") +
  scale_color_manual(values = c("blue", "red"), 
                     labels = c("Non-seasonally Adjusted", "Seasonally Adjusted"))
```


## (1 point) Task 2b: Compare linear model forecasts against realized CO2

Descriptively compare realized atmospheric CO2 levels to those predicted by your forecast from a linear time model in 1997 (i.e. "Task 2a"). (You do not need to run any formal tests for this task.) 

The actual monthly mean CO2 levels has a systematic increasing trend and regular fluctuations in fixed time period, indicating consistent growth and seasonality. The decomposition further proved its non-stationarity, increasing trend, and seasonality. In previous sections, we used linear model with quadratic term and season to capture the increasing rate and seasonality and predict the CO2 till 2022 Dec. The figure shows that the peak of 2020 is slightly below 6.05, while the peak of 2022 is over 6.05. In actual data plot, we can found that the peak of 2020 is around 6.03, while th peak of 2022 still below 6.05. Therefore, we can say the linear model was able to predict the seasonality in the predicted data but the systematic increasing trend was slightly over estimated, indicating higher slope coefficient in predicted trend than the actual trend of realized atmospheric CO2 data.  


```{r,fig.width = 12, fig.height = 4, fig.align = 'center', warning = F}
actual<-monthly_mean_co2_df %>%
  pivot_longer(cols = c(co2, sa_co2), 
               names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = time_index, y = log(value), color = variable)) +
  geom_line() +
  labs(title = "Plot of Seasonally Adjusted CO2 and Non-seasonally Adjusted CO2",
       x = "Date",
       y = "log CO2 part per million") +
  scale_color_manual(values = c("blue", "red"), 
                     labels = c("Non-seasonally Adjusted", "Seasonally Adjusted"))


mod.quad_season.predictions <- new_data(co2_df, n = 300)

linear_predict<-mod.quad_season %>%
  forecast(new_data = mod.quad_season.predictions) %>%
  autoplot(co2_df) + labs(title = "Linear time trend model Forecast")

linear_predict | actual 
```

## (1 point) Task 3b: Compare ARIMA models forecasts against realized CO2  

Descriptively compare realized atmospheric CO2 levels to those predicted by your forecast from the ARIMA model that you fitted in 1997 (i.e. "Task 3a"). Describe how the Keeling Curve evolved from 1997 to the present. 

The ARIMA model we obtained in previous sections predicts that the CO2 will have a steady increasing trend with period fluctuation. However, contrary to linear model, the ARIMA model seems underestimate the CO2 increase by predicting the the peak of 2020 is slightly over 400, while the peak of 2022 is around 405. In actual data plot, we can found that the peak of 2020 is much over 410 even approaching 420 and the peak of 2022 is over 420. Therefore, we can say the ARIMA model was able to predict the seasonality and increasing trend in the predicted data but the trend is underestiamte, indicating lower slope coefficient in predicted trend than the actual trend of realized atmospheric CO2 data.  

```{r forecast, fig.width = 12, fig.height = 4, fig.align = 'center', warning = F}
ARIMA<-co2_fit%>%
  forecast(h=300)%>%
  autoplot()+ labs(title = "Figure 11 ARIMA Model Forecast")

actual<-monthly_mean_co2_df %>%
  pivot_longer(cols = c(co2, sa_co2), 
               names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = time_index, y = value, color = variable)) +
  geom_line() +
  labs(title = "Plot of Seasonally Adjusted CO2 and Non-seasonally Adjusted CO2",
       x = "Date",
       y = "log CO2 part per million") +
  scale_color_manual(values = c("blue", "red"), 
                     labels = c("Non-seasonally Adjusted", "Seasonally Adjusted"))

ARIMA | actual 
```

## (3 points) Task 4b: Evaluate the performance of 1997 linear and ARIMA models 

In 1997 you made predictions about the first time that CO2 would cross 420 ppm. How close were your models to the truth? 

After reflecting on your performance on this threshold-prediction task, continue to use the weekly data to generate a month-average series from 1997 to the present, and compare the overall forecasting performance of your models from Parts 2a and 3b over the entire period. (You should conduct formal tests for this task.) 

In linear model, the first time that CO2 cross 420 ppm is 2022 Jan, while in actual data it is 2022 Apr. The ARIMA model did not predict CO2 could cross 420 ppm in 2022. Apparently, linear model is more close to the actual data. The time series plots are used to show the difference between actual minus predicted data. Consistent with above discussion, the linear model tends to over estimate the CO2 and the degree of overestimation tends to increase with the time evolve, while ARIMA model tends to underestimate the CO2 and the degree of underestimation tends to increase with the time evolve. However, ARIMA model has more smooth and consistent underestimation in residual, while linear model's prediction has bigger fluctuation in residual. From the histogram, we can tell that most residual in linear model lie between [-3,-1], while ARIMA model has a bigger range, [0.5,6]. Finally, we use accuracy function to test the gap between predicted and observed values. The results show that linear model has smaller gap (RMSE=2.14) than ARIMA model(RMSE=8.09), indicating better model fit.

```{r forecast evaluation,fig.width = 12, fig.height = 4, fig.align = 'center', warning = F}
linear_predict_df<-mod.quad_season %>%
  forecast(new_data = mod.quad_season.predictions)%>%
  mutate(linear_co2=exp(.mean))

ARIMA_predict_df<-co2_fit%>% forecast(h=300) %>%
  mutate(arima_co2=.mean)


comps<-left_join(linear_predict_df,monthly_mean_co2_df,by="time_index")
comps<-left_join(comps,ARIMA_predict_df, by="time_index")

comps<-comps%>%mutate(linear_diff=co2.x-linear_co2,
                      arima_diff=co2.x-arima_co2)

comps2<- comps%>%
  as_tsibble(index = time_index) 

# TS
timeplot_linear_diff<- comps2 %>%
  ggplot() +
  aes(x=time_index, y=linear_diff) +
  geom_line() +
  labs(
    title = "Difference between Actual-Linear Model Prediction",
    x = "Time Index",
    y = "Difference"
  )

timeplot_arima_diff<- comps2 %>%
  ggplot() +
  aes(x=time_index, y=arima_diff) +
  geom_line() +
  labs(
    title = "Difference between Actual-ARIMA Model Prediction",
    x = "Time Index",
    y = "Difference"
  )

# Hist
hist_linear_diff <- comps2 %>%
  ggplot() +
  geom_histogram(aes(x = linear_diff)) +
  labs(
    title = "Linear Model CO_2 Differfence Histogram",
    x = "Difference"
  ) +
  theme(legend.position = c(.2, .8))

hist_arima_diff <- comps2 %>%
  ggplot() +
  geom_histogram(aes(x = arima_diff)) +
  labs(
    title = "ARIMA Model CO_2 Differfence Histogram",
    x = "Difference"
  ) +
  theme(legend.position = c(.2, .8))

(timeplot_linear_diff/
    timeplot_arima_diff)

(hist_linear_diff/
    hist_arima_diff)

#Accuracy to compare the performance of two models

data.frame(Model=c("Linear Model Accurarcy","ARIMA Model Accurarcy"),
           rbind(accuracy(comps2$linear_co2, comps2$co2.x),
           accuracy(comps2$arima_co2, comps2$co2.x)))


```


## (4 points) Task 5b: Train best models on present data

Seasonally adjust the weekly NOAA data, and split both seasonally-adjusted (SA) and non-seasonally-adjusted (NSA) series into training and test sets, using the last two years of observations as the test sets. For both SA and NSA series, fit ARIMA models using all appropriate steps. Measure and discuss how your models perform in-sample and (psuedo-) out-of-sample, comparing candidate models and explaining your choice. In addition, fit a polynomial time-trend model to the seasonally-adjusted series and compare its performance to that of your ARIMA model.

```{r EDA - plots, monthly_mean_co2_df, non-seasonally adjust, warning = F}
# Time plot
timeplot_co2_m.p <- monthly_mean_co2_df %>%
  ggplot() +
  aes(x=time_index, y=co2) +
  geom_line() +
  labs(
    title = "Monthly Mean CO_2",
    x = "Time",
    y = "CO_2 Parts per Million"
  )

# Histogram
hist_co2_m.p <- monthly_mean_co2_df %>%
  ggplot() +
  geom_histogram(aes(x = co2)) +
  labs(
    title = "Histogram of Monthly CO_2",
    x = "CO_2 Parts per Million"
  ) +
  theme(legend.position = c(.2, .8))

# ACF Plot
acf_co2_m.p <- monthly_mean_co2_df %>%
  ACF(y=co2) %>%
  autoplot()

# PACF Plot
pacf_co2_m.p <- monthly_mean_co2_df %>%
  ACF(y=co2, type = "partial") %>%
  autoplot() +
  labs(y = "PACF")

(timeplot_co2_m.p + hist_co2_m.p) /
  (acf_co2_m.p + pacf_co2_m.p)
```
```{r EDA - plots, monthly_mean_co2_df, seasonally adjust, warning = F}
# Time plot
timeplot_co2_m_sa.p <- monthly_mean_co2_df %>%
  ggplot() +
  aes(x=time_index, y=sa_co2) +
  geom_line() +
  labs(
    title = "Monthly Mean of Seasonally Adjusted CO_2",
    x = "Time",
    y = "CO_2 Parts per Million"
  )

# Histogram
hist_co2_m_sa.p <- monthly_mean_co2_df %>%
  ggplot() +
  geom_histogram(aes(x = sa_co2)) +
  labs(
    title = "Histogram of Monthly Seasonally Adjsuted CO_2",
    x = "CO_2 Parts per Million"
  ) +
  theme(legend.position = c(.2, .8))

# ACF Plot
acf_co2_m_sa.p <- monthly_mean_co2_df %>%
  ACF(y=sa_co2) %>%
  autoplot()

# PACF Plot
pacf_co2_m_sa.p <- monthly_mean_co2_df %>%
  ACF(y=sa_co2, type = "partial") %>%
  autoplot() +
  labs(y = "PACF")

(timeplot_co2_m_sa.p + hist_co2_m_sa.p) /
  (acf_co2_m_sa.p + pacf_co2_m_sa.p)
```
```{r, difference of sa_co2}
monthly_mean_co2_df %>%
  ACF(y=difference(sa_co2), type = "partial") %>%
  autoplot() +
  labs(y = "PACF")
```


```{r, ARIMA Model}
# Split original monthly_mean_co2 dataframe to training and testing set
train_monthly <- subset(monthly_mean_co2_df, year < 2023)
test_monthly <- subset(monthly_mean_co2_df, year >= 2023)

# Iterate through different ARIMA models
##### Non-seasonally adjusted series #####
#train_monthly$time_index <- as.Date(train_monthly$time_index, "%Y=%m")
model_fit1 <- train_monthly %>%
  model(model1 = ARIMA(co2 ~ 0 + pdq(0:3,1:2,0:3) + PDQ(0:3,0:1,0:3, period=12), 
                       ic="bic", stepwise=F, greedy=F))
model_fit1 %>% report()
model_fit1 %>% coef()

# Residual stationary diagnosis
model_fit1 %>% gg_tsresiduals()+ labs(title = "Model Residual")
model_fit1 %>% resid() %>%
  as.ts()%>%
  Box.test(., lag=10, type="Ljung-Box")
```
Based on the EDA of first differencing data, we can observe that the mean of the first differenced $CO_2$ is fluctuated around zero. Thus, we set intercept to be equal 0 and parameter D to range from 1 to 2. The model with minimum BIC is `ARIMA(1,1,1)(2,1,1)[12]`. Although only `ma1` and `sma1` terms are sarcastically significant, based on the time plot and ACF, PACF plots in EDA section, we can observe strong and  persistent non-seasonal and seasonal trend.

The residual plots and KPSS test result suggest that the residuals of the model is stationary.

```{r, forecast for non-seasonally adjusted series}
model.forecasts1 <- forecast(model_fit1, h=nrow(test_monthly))

# plot
model.forecasts1 %>%
  autoplot(colour="cornflowerblue") +
  autolayer(monthly_mean_co2_df, colour="black", .vars = co2) +
  geom_line(data=model_fit1 %>% augment(), aes(time_index, .fitted, color=.model)) +
  labs(title = "model.fit1: ARIMA(1,1,1)(2,1,1)[12]") +
  facet_wrap(~.model, ncol=1, nrow=3)

# forecast fit
forecast.fit1 <- model.forecasts1$.mean

# accuracy
acc1 <- data.frame(accuracy(forecast.fit1, test_monthly$co2))
acc1
```

COMMENT: compare to models without PDQ termss
```{r}
##### Seasonally adjusted series #####
model_fit2 <- train_monthly %>%
  model(model2 = ARIMA(sa_co2 ~ 0 + pdq(0:3,0:2,0:3) + PDQ(0:3,0:1,0:3, period=12), 
                       ic="bic", stepwise=F, greedy=F))
model_fit2 %>% report()
model_fit2 %>% coef()
```

```{r, forecast for seasonally adjusted series}
model.forecasts2 <- forecast(model_fit2, h=nrow(test_monthly))

# plot
model.forecasts2 %>%
  autoplot(colour="cornflowerblue") +
  autolayer(monthly_mean_co2_df, colour="black", .vars = sa_co2) +
  geom_line(data=model_fit2 %>% augment(), aes(time_index, .fitted, color=.model)) +
  labs(title = "model.fit2: ARIMA(0,1,1)(3,0,0)[12]") +
  facet_wrap(~.model, ncol=1, nrow=3)

# forecast fit
forecast.fit2 <- model.forecasts2$.mean

# accuracy
acc2 <- data.frame(accuracy(forecast.fit2, test_monthly$sa_co2))
acc2
```


```{r, Polynomial Model}

# Iterate through different Polynomial models
##### Seasonally adjusted series #####
mod.quad_sa1 <- train_monthly %>%
  model(trend_model = TSLM(sa_co2 ~ trend() + I(trend()^2))) 
mod.quad_sa1 %>% gg_tsresiduals() + labs(title="Polynomial Model1's Residuals")

fit_acc1 <- accuracy(mod.quad_sa1, type = "fitted")

mod.quad_sa2 <- train_monthly %>%
  model(trend_model = TSLM(sa_co2 ~ trend() + I(trend()^2) + I(trend()^3))) 
mod.quad_sa2 %>% gg_tsresiduals() + labs(title="Polynomial Model2's Residuals")

fit_acc2 <- accuracy(mod.quad_sa2, type = "fitted")

fit_acc1 
fit_acc2
```
The training accuracy of polynomial model with order 2 is similar to that of polynomial model with order 3.
```{r, forecast for seasonally adjusted series}
model.forecasts.poly1 <- forecast(mod.quad_sa1, h=nrow(test_monthly))

# plot
model.forecasts.poly1 %>%
  autoplot(colour="cornflowerblue") +
  autolayer(monthly_mean_co2_df, colour="black", .vars = sa_co2) +
  geom_line(data=mod.quad_sa1 %>% augment(), aes(time_index, .fitted, color=.model)) +
  labs(title = "mod.quad_sa2: polynomial(x + x^2)") +
  facet_wrap(~.model, ncol=1, nrow=3)

# forecast fit
forecast.fit.poly1 <- model.forecasts.poly1$.mean

# accuracy
acc1.p <- data.frame(accuracy(forecast.fit.poly1, test_monthly$sa_co2))
acc1.p 
```
Polynomial model generates more accurate forecasting results for seasonally adjusted $CO_2$ trend compared to ARIMA.
```{r}
acc_df <- rbind(acc1, acc2, acc1.p)
row.names(acc_df) <- c("NSA ARIMA", "SA ARIMA", 
                  "SA Polynomial")
acc_df
```

## (3 points) Task Part 6b: How bad could it get?

With the non-seasonally adjusted data series, generate predictions for when atmospheric CO2 is expected to be at 420 ppm and 500 ppm levels for the first and final times (consider prediction intervals as well as point estimates in your answer). Generate a prediction for atmospheric CO2 levels in the year 2122. How confident are you that these will be accurate predictions?

Based on the non-seasonally adjusted data series, we used the non-seasonally adjusted ARIMA model to generate predictions for atmospheric CO2 levels until the year 2122.

```{r}
# Generate forecast till end of 2100
NSAfit.forecast <- model_fit1 %>% forecast(h=1200)

# Plot the forecasting results
NSAfit.forecast %>%
  autoplot()+ labs(title = "Figure 13 Final Model Forecast")
```

Using a 95% confidence intervals we noted the first and last years that we would expect the atmospheric CO2 levels to be at 420 ppm and 500 ppm. The following month and years were calculated:

```{r}
confidence_intervals <- NSAfit.forecast %>%
  summarise(.mean, .lower = quantile(.mean, 0.025), .upper = quantile(.mean, 0.975))

# Extract lower confidence intervals
lower_confidence_intervals <- confidence_intervals$.lower
upper_confidence_intervals <- confidence_intervals$.upper

# Find the indices where the lower confidence intervals are greater than 420 and 500ppm and
# upper confidence intervals are less than 420 and 500ppm
indices_above_420 <- which(lower_confidence_intervals > 420)
indices_above_500 <- which(lower_confidence_intervals > 500)
indices_below_420 <- which(upper_confidence_intervals < 420)
indices_below_500 <- which(upper_confidence_intervals < 500)

first_above_420_index <- min(indices_above_420)
first_above_500_index <- min(indices_above_500)
last_below_420_index <- max(indices_below_420)
last_below_500_index <- max(indices_below_500)

# Extract the corresponding month and year
first_above_420_monthyear <- confidence_intervals$time_index[first_above_420_index]
first_above_500_monthyear <- confidence_intervals$time_index[first_above_500_index]
last_below_420_monthyear <- confidence_intervals$time_index[last_below_420_index]
last_below_500_monthyear <- confidence_intervals$time_index[last_below_500_index]

# View the result
print(paste("With 95% confidence, the first month and year where lower confidence interval exceeds 420ppm:", first_above_420_monthyear))
print(paste("With 95% confidence, the last month and year where upper confidence interval subceeds 420ppm:", last_below_420_monthyear))
print(paste("With 95% confidence, the first month and year where lower confidence interval exceeds 500ppm:", first_above_500_monthyear))
print(paste("With 95% confidence, the last month and year where upper confidence interval subceeds 500ppm:", last_below_500_monthyear))
```
